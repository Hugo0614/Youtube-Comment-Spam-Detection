{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fd0ea7a-27db-412c-8e47-f5c0c2793288",
   "metadata": {},
   "source": [
    "# ***Youtube Comment Spam Detection - Report***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b93e5d9-eb4d-4a75-b056-2cdd824c159b",
   "metadata": {},
   "source": [
    "## **Name: Chau Ho San**\n",
    "## **EID: 57150406**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b138c6-ff05-4962-b701-be7b98be3ef4",
   "metadata": {},
   "source": [
    "### **Background** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46728ba5-8750-4550-8f1f-ae42c37f466f",
   "metadata": {},
   "source": [
    "The project revolves around the prevalent issue of spam comments on YouTube videos, which significantly diminishes the user experience and authenticity of content. These spam comments, often promotional or irrelevant in nature, inundate the comment sections, making it arduous for users to discover meaningful discussions or engage with content creators. This project aims to tackle this problem by developing a classifier capable of discerning between spam and genuine comments. The ultimate goal is to create a dependable tool that can effectively identify spam comments, thereby enhancing the quality of user interactions on YouTube and fostering a more positive and engaging online environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc42f00-7801-4b99-b74d-85c24f874189",
   "metadata": {},
   "source": [
    "### **Method**   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0a7acf-8137-4902-adb8-8d584b8b5927",
   "metadata": {},
   "source": [
    "\n",
    "#### Model Architecture and Components\n",
    "\n",
    "In this project, the core of the methodology is based on the implementation of a **Long Short-Term Memory (LSTM) neural network**. I chose this model because it is exceptionally well-suited for processing sequential data, such as the text found in YouTube comments. Due to its ability, I can use it to capture temporal dependencies and contextual nuances within the data. The architecture of the LSTM model is meticulously designed and also can add several layers to the model which we need to optimize the model's performance:\n",
    "\n",
    "#### Detailed Description of Model Layers\n",
    "\n",
    "1. **Embedding Layer**:\n",
    "   - **Purpose**:This layer is used as an entry point for the input text data and transforms the text into a dense vector representation. Its main purpose is to transform sparse high-dimensional word indexes into a low-dimensional continuous vector space, thus enabling semantically similar words to be close to each other.\n",
    "   - **Details**. The vectors are learned during the training process and become more refined to reflect the nuances of the language used in YouTube comments.\n",
    "<br><br>\n",
    "2. **Batch Normalization Layer**:\n",
    "   - **Purpose**: Its main function is to stabilise the learning process by normalising the batch data, allowing higher learning rates and faster convergence. This layer adjusts the activation values of the embedding layer by normalising them to have a mean of zero and a variance of one, which helps to combat the problem of internal covariate bias.\n",
    "   - **Details**: By normalising the data, the model becomes less sensitive to the specific initialisation of the weights and can be trained more consistently and efficiently.\n",
    "<br><br>\n",
    "3. **LSTM Layer**:\n",
    "   - **Purpose**: This layer is the core of the model and it deals with sequences of embedded vectors. It specialises in capturing temporal dependencies and contextual relationships in text, which has very important implications for understanding the flow and intent of comments.\n",
    "   - **Capabilities**: LSTM is designed to remember information over long periods of time. As an example, if a comment is adding some spam text to normal content, LSTM can help distinguish between spam comments with cleverly inserted spam content. The layer uses gates to control the flow of information, maintaining memory of past important inputs while forgetting irrelevant data.\n",
    "<br><br>\n",
    "4. **Dropout Layer**:\n",
    "   - **Purpose**: This layer is designed to prevent the common overfitting problem encountered in deep learning models. By randomly disabling a portion of the neurons during the training phase, it forces the network to not rely on any one neuron, thus promoting more decentralised and robust feature learning.\n",
    "   - **Details**: Giving a proportion of randomness helps to enhance the generalisation of the model.s of the model.\n",
    "<br><br>\n",
    "5. **Dense Layer with Sigmoid Activation**:\n",
    "    -  **Purpose**: As the final layer of the model, it integrates the features learnt by the LSTM into a single output prediction. The output of this layer is mapped to a probability score between 0 and 1 using a sigmoid activation function, which represents the likelihood that the model will evaluate the text to determine whether it is spam or not after inputting a comment.t comment is spam.\n",
    "    - **Details**: When the task is binary classification (which is what this project is trying to achieve by determining whether the content of a YouTube comment is spam or non-spam), it consists of a single neuron. The output of the sigmoid function, on the other hand, is well suited for binary decision making, as it provides an explicit threshold of 0.5 to categorise the comments.\n",
    "<br><br>\n",
    "\n",
    "Each of the layers in this LSTM model plays an important role in dealing with the classification of complex textual data, especially when it comes to detecting spam in YouTube comments. Combining these layers allows the model to effectively learn from the data and make informed predictions about the nature of each comment.\n",
    "<br><br>\n",
    "#### Optimization and Training Enhancements\n",
    "\n",
    "- **Adam Optimizer**: The Adam Optimizer has an excellent adaptive learning rate feature, using which the learning rate can be adjusted throughout the training process. This feature is particularly beneficial in dealing with the complexity and non-stationarity of text data, ensuring faster and more efficient convergence.    \n",
    "\n",
    "- **Early Stop Mechanism**: An early stop callback function is integrated into the training process to monitor the model's performance on the validation set. If the performance does not improve within a predefined number of calendar hours, training is stopped. Using this approach not only helps prevent overfitting, but also saves computational resources by stopping training when the model reaches an optimal point during training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c5be4d-4ed2-4971-a23f-10b32a39390b",
   "metadata": {},
   "source": [
    "### **Experiments** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6438cb96-120c-46d0-88ae-f6f64de8f770",
   "metadata": {},
   "source": [
    "- Import all specific libraries and modules that can provide the necessary tools and functions to efficently accomplish tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96738fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "#import tensorflow_addons as tfa\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Dense, Dropout, Embedding, LSTM, BatchNormalization\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d16225c-efe2-416f-adf3-4fc541f6d2f0",
   "metadata": {},
   "source": [
    "- The inputs of the model:\n",
    "\n",
    "  Read the training data from the .csv file and the testing data from the .xlsv file. The training dataset is derived from CSV files and consists of comment text, author's name, comment date, video ID and category labels (0 for non-spam comments and 1 for spam comments). In this project, only two columns of data are actually useful for training and testing the model, namely the \"CLASS\" and \"TEXT\". This means that when I need to read and extract data from the two files, I only need to consider the aforementioned two columns.\n",
    "\n",
    "  Therefore, we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ed5d0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the train .csv file and just take two useful col TEXT & CLASS\n",
    "def read_csv_file(file_path):\n",
    "    data = pd.read_csv(file_path)\n",
    "    text = data['TEXT']\n",
    "    labels = data['CLASS']\n",
    "    return text, labels\n",
    "\n",
    "# read the train .xlsv file and just take two useful col TEXT & CLASS\n",
    "def read_xlsv_file(file_path):\n",
    "    data = pd.read_excel(file_path)\n",
    "    text = data['TEXT']\n",
    "    labels = data['CLASS']\n",
    "    return text, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84239ab7-0c0a-438d-8bb6-aba60206a718",
   "metadata": {},
   "source": [
    "- For the training part, this function **train_model_Adam(text, labels, num_epochs)** is to build and trains a text classification model using Adam optimizer.\n",
    "\n",
    "  1. **Tokenize the text data**: The first part, `Tokenizer()` converts text data into sequences of integers, and each integer represents a specific word from the dataset.\n",
    "     \n",
    "  2. **Padding sequences to have the same length**: The padding part is used to ensure all text sequences have the same length (Using `maxlen=max_sequence_length` to make every text sequence have the same length, which is equal to the maximum length of all text comments). This part is important because most deep learning models require input data of consistent size.\n",
    " \n",
    "  3. **Splitting into training and testing sets**:\n",
    "     - To divide the data into training and testing sets, I chose `train_size=0.8`, which selects 80% of the data for training and the remaining data for testing. There are multiple benefits to selecting a larger `train_size`. More training data enhances the model's ability to discern intricate patterns, boosting its overall accuracy. It also mitigates the risk of overfitting, ensuring that the model generalizes well to unseen data rather than merely memorizing the training examples. Additionally, a larger dataset enhances the model's robustness by encompassing a wider variety of real-world scenarios, which improves its practical applicability. Moreover, a substantial training set increases statistical reliability, lending more credibility to the model's predictions.\n",
    "     - Setting `random_state=42` guarantees that the split is consistent, ensuring that the same data points are allocated to the training and test sets in each run.\n",
    "\n",
    "  4. **Model Architecture**:\n",
    "     - `Embedding`: Converts integer sequences into dense vectors of fixed size.\r\n",
    "       - `input_dim=len(tokenizer.word_index) + 1`: The input dimension is set to the number of unique words + 1 (for the padding zero).\r\n",
    "       - `output_dim=100`: Each word is represented by a vector of length 100.\r\n",
    "       - `input_length=max_sequence_length`: Each input sequence will have this fixed length.\r\n",
    "     - `BatchNormalization`: Normalizes the activations of the previous layer, which helps in speeding up training and reducing overfitting.\r\n",
    "     - `LSTM(128)`: A Long Short-Term Memory layer with 128 units to process sequences.\r\n",
    "     - `Dropout(0.5)`: Randomly sets input units to 0 with a frequency of 50% at each step during training time, which helps to prevent overfitting.\r\n",
    "     - `Dense(1, activation='sigmoid')`: Outputs a single value between 0 and 1, representing the probability of the target class (binary classific\n",
    "\n",
    "\n",
    "    5. **Model Compilation**: I chose `Adam` as the optimizer and its `learning_rate` is equal to `0.0001`. A relatively small learning rate is selected to enable the model to learn incrementally, ensuring it doesn't overlook minima on the loss surface. To choose the `loss`, `binary_crossentropy` is the most suitable for binary classification tasks.\n",
    "\n",
    "\n",
    "    6. **Training**: Using the fit method to train the model\n",
    "       - `batch_size=32`: Number of samples per gradient update. Common sizes are 32, 64, and 128. Because there are not too many samples, 32 size is enough.\r",
    "        - `epochs=num_epochs`: Allows flexibility to set the number of training epochs externally.\r\n",
    "       - `verbose=1`: Shows a progress bar during training.\r\n",
    "       - `validation_split=0.2`: Uses 20% of the training data as validation data.\r\n",
    "       - `callbacks=[early_stopping]`: Stops training when the validation loss has not improved for 10 epochs (`patience=10`) and restores model weights from the epoch with the best value of the monitored quantity (`restore_best_weights=Tru\n",
    "\n",
    "In the end, the trained `loss` and `training set accuracy` data is printed and returned to the `model`, `tokenizer`, and training `history`.`).\r\n",
    "tion). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eab2bc79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_Adam(text, labels, num_epochs):\n",
    "    # Tokenize the text data\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(text)\n",
    "    sequences = tokenizer.texts_to_sequences(text)\n",
    "\n",
    "    # Padding sequences to have the same length\n",
    "    max_sequence_length = max([len(seq) for seq in sequences])\n",
    "    padded_sequences = pad_sequences(sequences, maxlen=max_sequence_length)\n",
    "\n",
    "    # Splitting into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(padded_sequences, labels, train_size=0.8, random_state=42)\n",
    "\n",
    "    # Model architecture\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=100, input_length=max_sequence_length))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LSTM(128))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    # Compile the model\n",
    "    optimizer = Adam(learning_rate=0.0001)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "\n",
    "    #history = model.fit(X_train, y_train, batch_size=32, epochs=num_epochs, verbose=1)\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "    history = model.fit(X_train, y_train, batch_size=32, epochs=num_epochs, verbose=1, validation_split=0.2, callbacks=[early_stopping])\n",
    "    evaluation = model.evaluate(X_test, y_test)\n",
    "    \n",
    "    print(\"Test set loss:\", evaluation[0])\n",
    "    print(\"Training set accuracy:\", evaluation[1])\n",
    "\n",
    "    return model, tokenizer, history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dfdb4d1-eefb-479d-8e30-bddbccf3207d",
   "metadata": {},
   "source": [
    "- This function **test_model(model, text, labels, tokenizer)** is designed to evaluate the performance of a pre-trained model by calculating its accuracy on a test dataset.\n",
    "  \n",
    "  1. **Tokenize the text data**:\n",
    "     - `texts_to_sequences(text)`: Converts the list of text data into sequences of integers.\r\n",
    "     - `pad_sequences(sequences, maxlen=model.input_shape[1])`:This process ensures uniformity in sequence length by padding the shorter ones, a critical step because neural networks need inputs of the same size. The parameter `maxlen=model.input_shape[1]` defines the maximum sequence length according to the modelâ€™s input dimensions, ensuring consistent input sizes and avoiding dimensional mismatches during prediction.\n",
    "\n",
    "  2. **Making predictions**:\n",
    "     - `model.predict(padded_sequences)`: Generates output predictions for the input sequences.\r\n",
    "     - `(predictions > 0.5).astype(int).flatten()`: Converts the probabilities returned by the model to binary labels (0 or 1). This is typical for binary classification tasks where a threshold of 0.5 is used to decide the class labels\n",
    "     -  `accuracy_score(labels, y_pred)`: Calculate the accuracy.\n",
    "    \n",
    "  3. **Creating a comparison Table**: `pd.DataFrame()` is to create a table which shows the text from the testing dataset, the predicted labels and the actual labels. This is helpful for manually reviewing whether the prediction cases are correct or not..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a11334c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the trained model and calculating accuracy\n",
    "def test_model(model, text, labels, tokenizer):\n",
    "    # Tokenize the text data\n",
    "    sequences = tokenizer.texts_to_sequences(text)\n",
    "    padded_sequences = pad_sequences(sequences, maxlen=model.input_shape[1])\n",
    "\n",
    "    # Making predictions using the model\n",
    "    predictions = model.predict(padded_sequences)\n",
    "    y_pred = (predictions > 0.5).astype(int).flatten()\n",
    "    accuracy = accuracy_score(labels, y_pred)\n",
    "    print(\"Testing set accuracy:\", accuracy)\n",
    "\n",
    "    # Create a table to compare the actual CLASS col and the predicted CLASS col\n",
    "    df = pd.DataFrame({'Text': text, 'Predicted Label': y_pred, 'Actual Label': labels})\n",
    "    print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e0eaa9-0315-4529-bb72-ff02c1e64232",
   "metadata": {},
   "source": [
    "- This function is used to generate a chart that displays the relationship between Loss and Epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "139bfc02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(history):\n",
    "    # Get the loss values from training history\n",
    "    loss = history.history['loss']\n",
    "\n",
    "    # Create a list of epochs\n",
    "    epochs = range(1, len(loss) + 1)\n",
    "\n",
    "    # Plot the loss values\n",
    "    plt.plot(epochs, loss, 'b', label='Training loss')\n",
    "    plt.title('Training Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1a8a4e-7dcf-47dc-ad8d-7a08e756e08d",
   "metadata": {},
   "source": [
    "### Result of Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e81ea2a-1c76-4122-9535-294dd3fcb9f5",
   "metadata": {},
   "source": [
    "Before I execute the program, based on what the model already does plus a variety of factors, and assuming that the large amount of data on the .csv archive is accurate enough and that there are valid features for each piece of text, I believe that with enough epochs, the accuracy of the prediction can reach at least 70% or more, and can stabilise at 85% or more."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fec8fed-3cab-44fc-a23f-dc06af2f0f76",
   "metadata": {},
   "source": [
    "- Read and get the training data (`TEXT` and `CLASS`) from `Topic1-youtube_spam_train.csv`, and set the `num_epochs` as 100 (can be changed to another larger integer). Then execute `train_model_Adam`; there is a process of every epoch which is displayed under the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5476c3c0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "16/16 [==============================] - 6s 180ms/step - loss: 0.6839 - accuracy: 0.5980 - val_loss: 0.6896 - val_accuracy: 0.5000\n",
      "Epoch 2/100\n",
      "16/16 [==============================] - 2s 139ms/step - loss: 0.6309 - accuracy: 0.6529 - val_loss: 0.6886 - val_accuracy: 0.4609\n",
      "Epoch 3/100\n",
      "16/16 [==============================] - 2s 142ms/step - loss: 0.5837 - accuracy: 0.7098 - val_loss: 0.6849 - val_accuracy: 0.5156\n",
      "Epoch 4/100\n",
      "16/16 [==============================] - 2s 143ms/step - loss: 0.5162 - accuracy: 0.7784 - val_loss: 0.6824 - val_accuracy: 0.5156\n",
      "Epoch 5/100\n",
      "16/16 [==============================] - 2s 145ms/step - loss: 0.4300 - accuracy: 0.8569 - val_loss: 0.6779 - val_accuracy: 0.5547\n",
      "Epoch 6/100\n",
      "16/16 [==============================] - 2s 147ms/step - loss: 0.3465 - accuracy: 0.9020 - val_loss: 0.6706 - val_accuracy: 0.6172\n",
      "Epoch 7/100\n",
      "16/16 [==============================] - 2s 152ms/step - loss: 0.2467 - accuracy: 0.9529 - val_loss: 0.6609 - val_accuracy: 0.6172\n",
      "Epoch 8/100\n",
      "16/16 [==============================] - 2s 148ms/step - loss: 0.1618 - accuracy: 0.9765 - val_loss: 0.6475 - val_accuracy: 0.6484\n",
      "Epoch 9/100\n",
      "16/16 [==============================] - 2s 147ms/step - loss: 0.1312 - accuracy: 0.9765 - val_loss: 0.6292 - val_accuracy: 0.7031\n",
      "Epoch 10/100\n",
      "16/16 [==============================] - 2s 151ms/step - loss: 0.1013 - accuracy: 0.9804 - val_loss: 0.6138 - val_accuracy: 0.7031\n",
      "Epoch 11/100\n",
      "16/16 [==============================] - 2s 149ms/step - loss: 0.0773 - accuracy: 0.9902 - val_loss: 0.5914 - val_accuracy: 0.7031\n",
      "Epoch 12/100\n",
      "16/16 [==============================] - 2s 150ms/step - loss: 0.0615 - accuracy: 0.9902 - val_loss: 0.5756 - val_accuracy: 0.6953\n",
      "Epoch 13/100\n",
      "16/16 [==============================] - 2s 148ms/step - loss: 0.0508 - accuracy: 0.9882 - val_loss: 0.5497 - val_accuracy: 0.7344\n",
      "Epoch 14/100\n",
      "16/16 [==============================] - 2s 150ms/step - loss: 0.0456 - accuracy: 0.9902 - val_loss: 0.5291 - val_accuracy: 0.7422\n",
      "Epoch 15/100\n",
      "16/16 [==============================] - 2s 150ms/step - loss: 0.0392 - accuracy: 0.9922 - val_loss: 0.5090 - val_accuracy: 0.7500\n",
      "Epoch 16/100\n",
      "16/16 [==============================] - 2s 151ms/step - loss: 0.0337 - accuracy: 0.9980 - val_loss: 0.4962 - val_accuracy: 0.7500\n",
      "Epoch 17/100\n",
      "16/16 [==============================] - 2s 153ms/step - loss: 0.0287 - accuracy: 0.9941 - val_loss: 0.4788 - val_accuracy: 0.7578\n",
      "Epoch 18/100\n",
      "16/16 [==============================] - 2s 152ms/step - loss: 0.0238 - accuracy: 0.9980 - val_loss: 0.4648 - val_accuracy: 0.7656\n",
      "Epoch 19/100\n",
      "16/16 [==============================] - 2s 152ms/step - loss: 0.0219 - accuracy: 0.9961 - val_loss: 0.4524 - val_accuracy: 0.7812\n",
      "Epoch 20/100\n",
      "16/16 [==============================] - 2s 155ms/step - loss: 0.0221 - accuracy: 0.9980 - val_loss: 0.4433 - val_accuracy: 0.7812\n",
      "Epoch 21/100\n",
      "16/16 [==============================] - 2s 156ms/step - loss: 0.0740 - accuracy: 0.9804 - val_loss: 0.4524 - val_accuracy: 0.7969\n",
      "Epoch 22/100\n",
      "16/16 [==============================] - 2s 154ms/step - loss: 0.0375 - accuracy: 0.9941 - val_loss: 0.4297 - val_accuracy: 0.8203\n",
      "Epoch 23/100\n",
      "16/16 [==============================] - 2s 153ms/step - loss: 0.0257 - accuracy: 0.9941 - val_loss: 0.3847 - val_accuracy: 0.8750\n",
      "Epoch 24/100\n",
      "16/16 [==============================] - 2s 155ms/step - loss: 0.0183 - accuracy: 0.9980 - val_loss: 0.3853 - val_accuracy: 0.8906\n",
      "Epoch 25/100\n",
      "16/16 [==============================] - 2s 157ms/step - loss: 0.0168 - accuracy: 0.9980 - val_loss: 0.3810 - val_accuracy: 0.8984\n",
      "Epoch 26/100\n",
      "16/16 [==============================] - 2s 156ms/step - loss: 0.0146 - accuracy: 1.0000 - val_loss: 0.3738 - val_accuracy: 0.9141\n",
      "Epoch 27/100\n",
      "16/16 [==============================] - 2s 155ms/step - loss: 0.0123 - accuracy: 1.0000 - val_loss: 0.3656 - val_accuracy: 0.9219\n",
      "Epoch 28/100\n",
      "16/16 [==============================] - 2s 152ms/step - loss: 0.0119 - accuracy: 1.0000 - val_loss: 0.3577 - val_accuracy: 0.9219\n",
      "Epoch 29/100\n",
      "16/16 [==============================] - 2s 154ms/step - loss: 0.0119 - accuracy: 1.0000 - val_loss: 0.3481 - val_accuracy: 0.9141\n",
      "Epoch 30/100\n",
      "16/16 [==============================] - 2s 155ms/step - loss: 0.0114 - accuracy: 1.0000 - val_loss: 0.3417 - val_accuracy: 0.9141\n",
      "Epoch 31/100\n",
      "16/16 [==============================] - 2s 154ms/step - loss: 0.0104 - accuracy: 1.0000 - val_loss: 0.3390 - val_accuracy: 0.9219\n",
      "Epoch 32/100\n",
      "16/16 [==============================] - 2s 155ms/step - loss: 0.0095 - accuracy: 1.0000 - val_loss: 0.3351 - val_accuracy: 0.9219\n",
      "Epoch 33/100\n",
      "16/16 [==============================] - 2s 155ms/step - loss: 0.0092 - accuracy: 0.9980 - val_loss: 0.3329 - val_accuracy: 0.9219\n",
      "Epoch 34/100\n",
      "16/16 [==============================] - 2s 154ms/step - loss: 0.0085 - accuracy: 1.0000 - val_loss: 0.3319 - val_accuracy: 0.9219\n",
      "Epoch 35/100\n",
      "16/16 [==============================] - 2s 154ms/step - loss: 0.0068 - accuracy: 1.0000 - val_loss: 0.3295 - val_accuracy: 0.9219\n",
      "Epoch 36/100\n",
      "16/16 [==============================] - 2s 155ms/step - loss: 0.0074 - accuracy: 1.0000 - val_loss: 0.3317 - val_accuracy: 0.9219\n",
      "Epoch 37/100\n",
      "16/16 [==============================] - 2s 156ms/step - loss: 0.0064 - accuracy: 1.0000 - val_loss: 0.3333 - val_accuracy: 0.9297\n",
      "Epoch 38/100\n",
      "16/16 [==============================] - 2s 154ms/step - loss: 0.0076 - accuracy: 1.0000 - val_loss: 0.3351 - val_accuracy: 0.9219\n",
      "Epoch 39/100\n",
      "16/16 [==============================] - 2s 153ms/step - loss: 0.0062 - accuracy: 1.0000 - val_loss: 0.3323 - val_accuracy: 0.9141\n",
      "Epoch 40/100\n",
      "16/16 [==============================] - 2s 154ms/step - loss: 0.0054 - accuracy: 1.0000 - val_loss: 0.3267 - val_accuracy: 0.9141\n",
      "Epoch 41/100\n",
      "16/16 [==============================] - 2s 152ms/step - loss: 0.0051 - accuracy: 1.0000 - val_loss: 0.3275 - val_accuracy: 0.9219\n",
      "Epoch 42/100\n",
      "16/16 [==============================] - 2s 153ms/step - loss: 0.0048 - accuracy: 1.0000 - val_loss: 0.3306 - val_accuracy: 0.9219\n",
      "Epoch 43/100\n",
      "16/16 [==============================] - 3s 164ms/step - loss: 0.0045 - accuracy: 1.0000 - val_loss: 0.3370 - val_accuracy: 0.9219\n",
      "Epoch 44/100\n",
      "16/16 [==============================] - 2s 156ms/step - loss: 0.0047 - accuracy: 1.0000 - val_loss: 0.3407 - val_accuracy: 0.9219\n",
      "Epoch 45/100\n",
      "16/16 [==============================] - 2s 156ms/step - loss: 0.0049 - accuracy: 1.0000 - val_loss: 0.3450 - val_accuracy: 0.9141\n",
      "Epoch 46/100\n",
      "16/16 [==============================] - 2s 153ms/step - loss: 0.0039 - accuracy: 1.0000 - val_loss: 0.3522 - val_accuracy: 0.9219\n",
      "Epoch 47/100\n",
      "16/16 [==============================] - 2s 153ms/step - loss: 0.0041 - accuracy: 1.0000 - val_loss: 0.3588 - val_accuracy: 0.9141\n",
      "Epoch 48/100\n",
      "16/16 [==============================] - 2s 154ms/step - loss: 0.0037 - accuracy: 1.0000 - val_loss: 0.3654 - val_accuracy: 0.9141\n",
      "Epoch 49/100\n",
      "16/16 [==============================] - 2s 157ms/step - loss: 0.0045 - accuracy: 1.0000 - val_loss: 0.3647 - val_accuracy: 0.9141\n",
      "Epoch 50/100\n",
      "16/16 [==============================] - 2s 154ms/step - loss: 0.0033 - accuracy: 1.0000 - val_loss: 0.3690 - val_accuracy: 0.9141\n",
      "5/5 [==============================] - 0s 68ms/step - loss: 0.2055 - accuracy: 0.9125\n",
      "Test set loss: 0.20549964904785156\n",
      "Training set accuracy: 0.9125000238418579\n"
     ]
    }
   ],
   "source": [
    "train_text, train_labels = read_csv_file(\"./Topic_3_Data/Topic1-youtube_spam_train.csv\")\n",
    "\n",
    "num_epochs = 100  \n",
    "\n",
    "model, tokenizer, history = train_model_Adam(train_text, train_labels, num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85eaf99e-d951-407a-9a79-6894b90d5380",
   "metadata": {},
   "source": [
    "The training process is stopped at **50** ecophs and its training set accuracy is about 91.3%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab8e623-8609-434a-bda8-c22e096f5f08",
   "metadata": {},
   "source": [
    "#### Chart of Training Loss\n",
    "\n",
    "Now let's see the chart of training loss. I used the `plot_loss(history)` function to display it.\n",
    "\n",
    "- The chart of **Training Loss**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "78390fca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the loss values\n",
    "plot_loss(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b4ec503-7355-4be3-819c-ca4ff1dd14a1",
   "metadata": {},
   "source": [
    "To summarise, the loss decreases gradually with increasing epoch and there is no significant rebound, which is a sign of a good training process, indicating that the model is gradually learning and improving its prediction accuracy. This relies on sound model design, appropriate choice of optimisation algorithm, proper learning rate setting and effective data processing strategy. It means that the model I designed meets the criteria I envisioned."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d4509f-978f-4997-9938-b8dcd6e6e571",
   "metadata": {},
   "source": [
    "#### Testing the model\n",
    "\n",
    "- Read the testing data from .xlsv file, also just read and get two columns data from the file (`Text` and `CLASS`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "afa8704b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 1s 71ms/step\n",
      "Testing set accuracy: 0.9455445544554455\n",
      "                                                  Text  Predicted Label  \\\n",
      "0    Is this the video that started the whole \"got ...                0   \n",
      "1                  Can anyone sub to my channel? :DÃ¯Â»Â¿                1   \n",
      "2                        prehistoric song..has beenÃ¯Â»Â¿                0   \n",
      "3    You think you're smart?        Headbutt your f...                0   \n",
      "4    DISLIKE.. Now one knows REAL music - ex. Enime...                0   \n",
      "..                                                 ...              ...   \n",
      "197                Check out this video on YouTube:Ã¯Â»Â¿                1   \n",
      "198                Check out this video on YouTube:Ã¯Â»Â¿                1   \n",
      "199                Check out this video on YouTube:Ã¯Â»Â¿                1   \n",
      "200                Check out this video on YouTube:Ã¯Â»Â¿                1   \n",
      "201  Ã°Å¸Å¡Â¨Ã°Å¸Å¡Â¨Ã°Å¸Å¡Â¨Ã°Å¸Å¡Â¨Ã°Å¸Å¡Â¨Ã°Å¸Å¡Â¨Ã°Å¸Å¡Â¨Ã°Å¸Å¡Â¨Ã°Å¸Å¡Â¨Ã°Å¸Å¡Â¨Ã°Å¸Å¡Â¨Ã°Å¸...                0   \n",
      "\n",
      "     Actual Label  \n",
      "0               0  \n",
      "1               1  \n",
      "2               0  \n",
      "3               0  \n",
      "4               0  \n",
      "..            ...  \n",
      "197             1  \n",
      "198             1  \n",
      "199             1  \n",
      "200             1  \n",
      "201             1  \n",
      "\n",
      "[202 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "test_text, test_labels = read_xlsv_file(\"./Topic_3_Data/Topic1-youtube_spam_test.xlsx\")\n",
    "\n",
    "test_model(model, test_text, test_labels, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6837424d-d8e7-4127-a191-fca9ea873f19",
   "metadata": {},
   "source": [
    "\n",
    "We can see that the accuracy is about **94.6%**. A model test accuracy of over 90 per cent indicates that it performs well in understanding and classifying the test data, effectively identifying and predicting patterns and trends in the data. This usually means that the model has good generalisation capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90dae4b-3017-4b6d-9852-2a690655a441",
   "metadata": {},
   "source": [
    "#### Get the relationship between epochs and accuracy of the model.\n",
    "\n",
    "- Since the number of epochs during model training is not fixed, and the resulting accuracy is not fixed either, I tried running the same code multiple times (repeating the execution of **train_model_Adam()**) to obtain multiple results.\n",
    "  \n",
    "  1. This is the accuracy of the training set and testing set to the model which is stopped at the 43rd epoch:\n",
    "     ![stop at 43rd](./Topic_3_Data/Training_stop_at_43_epoch.jpg)\n",
    "     ![stop at 43rd](./Topic_3_Data/Adam_Testing_result_43_epoches.jpg)\n",
    "\n",
    "  2. This is the accuracy of the training set and testing set to the model which is stopped at the 53rd epoch:\n",
    "     ![stop at 53rd](./Topic_3_Data/Training_stop_at_53_epoch.jpg)\n",
    "     ![stop at 53rd](./Topic_3_Data/Adam_Testing_result_53_epoches.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7af5d9-0c4a-45c9-b401-208d85c16e7c",
   "metadata": {},
   "source": [
    "Look at these images. We can find a rule:  The accuracy obtained from either **Training** or **Testing** is deeply related to how many epochs were performed during training. Usually, the more epochs are run when training a model, the higher the accuracy will be get!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1ceed7-646e-4532-98f0-f9270c3ed6a4",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a4ac4d-e3f3-4b41-a4b4-84268741cce3",
   "metadata": {},
   "source": [
    "This project successfully illustrates the construction and training of neural networks for text classification, tackling prevalent issues like overfitting with strategies including dropout and early stopping. Tokenization and padding are employed to standardize the input data, whereas LSTM layers are utilized to capture dependencies within sequential data. Overall, the project establishes a comprehensive pipeline from data ingestion and preprocessing to the training, evaluation, and application of neural networks for practical text classification scenarios."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
